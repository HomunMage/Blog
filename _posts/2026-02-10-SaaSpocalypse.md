---
title:  "SaaSpocalypse Proves Andrej Karpathy Was a Prophet: RLVR Isn't Just Theory, It's Eating the Software Industry"
date:   2026-02-10 10:00:00 +0800
tags: [Programming]
---

## When Wall Street Lost $830 Billion, That "Ghost Living in the Terminal" Went From Theory to Reality

---

On February 3, 2026, something strange happened on Wall Street.

Anthropic released a few small tools. No new model, no breakthrough announcement. Just a few plugins that let Claude handle legal, financial, and sales work.

Then software stocks collapsed across the board.

Thomson Reuters plunged 15.83% in a single day, the worst day in company history. LegalZoom dropped nearly 20%. Salesforce, Adobe, ServiceNow—the entire software industry fell like dominoes.

In six days, **$830 billion** evaporated.

The media gave this disaster a name: **"SaaSpocalypse"**—the apocalypse of SaaS.

But this wasn't panic. This was Wall Street suddenly realizing something:

**Everything Andrej Karpathy said six months ago, that "crazy talk," was true.**

---

## First, a Story: Karpathy Said AI Is a Ghost

At the end of 2025, former Tesla AI Director and OpenAI founding member Andrej Karpathy said something in his year-end review that many people didn't take seriously at the time:

> **"AI is Ghost, not Creature."**

What does this mean?

The traditional view holds that truly intelligent AI needs to "live in the world" like humans—it needs eyes to see things, hands to touch things, to roll around in real environments learning. Like a child who has to touch fire to know it's hot.

Karpathy said no.

He said AI is more like a "ghost"—it doesn't need a physical body, doesn't need to live in the physical world. It can exist in a completely different way: understanding the world through text, code, and symbols.

Sounds mystical, right?

But Claude Cowork proved this.

---

## Claude Learned to Work in the "Symbolic World"

Let me explain what Claude Cowork actually does.

It's not an ordinary chatbot. It's an assistant that can "do things on its own" inside your computer:

- It reads your documents
- Understands relationships between documents
- Executes a series of work steps on its own
- Adjusts its strategy based on results

Here's the key point: **It doesn't need to "see" the real world at all.**

Reviewing contracts? It doesn't need to know what paper looks like, just understand the logic of legal terms.

Financial analysis? It doesn't need to visit the company in person, just understand how report numbers connect.

This is what Karpathy meant by "ghost"—it lives in a world made purely of symbols, and in this world, it learns incredibly fast.

Why? Because in the symbolic world, right and wrong are crystal clear. Does the code run or not? Does the contract have loopholes? Are the numbers calculated correctly? Everything has a definite answer.

This is far more efficient than learning in the physical world. You don't need to spend twenty years teaching AI to drive; you just let it practice in an environment where "rules are clear and answers are definite."

---

## But What's Really Impressive: Claude Isn't That "Big"

Here's a detail everyone overlooked.

Claude Cowork has a core design called **Skills**. Here's how Anthropic officially describes it:

> "Skills are procedural knowledge—instructions for completing specific tasks."
>
> "When an Agent starts, it only loads each Skill's name and description. It only reads the full content when the task requires it."

What does this mean?

**Claude itself doesn't need to "know" how to review contracts or do financial analysis.**

It only needs to know one thing: **When to use which tool.**

Like a smart new hire who doesn't need to memorize all company procedures, but knows "this issue needs to go to legal" and "that report needs to be pulled from this system."

This is exactly the architecture Karpathy described:

```
Traditional approach:
Train a super-brain, stuff all knowledge into it
→ Model gets fatter, training gets more expensive, updates get harder

Ghost approach:
Train a small, precise "judgment core"
+ Put a bunch of "toolkits" beside it that can be called anytime
→ The core handles "judging when to use what," the toolkits handle "how to do it"
```

This "ghost" might be much smaller than you imagine.

What really makes it powerful isn't how much is stuffed in its brain, but that it knows how to use tools.

---

## Think About It—Human Experts Work the Same Way

A great lawyer doesn't memorize all legal codes. But they know how to search regulations, how to use legal databases, what precedents to look for in what situations.

A strong engineer doesn't remember all programming syntax. But they know how to check documentation, how to use Stack Overflow, what keywords to Google for what problems.

The core capability of human experts has never been "possessing knowledge," but "knowing how to find and apply knowledge."

Claude's design perfectly replicates this pattern:

- LLM core = Judgment (when do you need what)
- Skills = Toolkits (how to do specific tasks)
- MCP connectors = Entry points to external systems (connecting CRM, databases, file systems)

So rather than saying Anthropic trained a "knows everything" super AI, it's more accurate to say they trained a "knows where to find everything" smart assistant.

And this assistant just caused Wall Street to lose $830 billion.

---

## Why Was the Market So Scared?

The research director at LPL Financial said something that hit everyone's pain point:

> "Why would I still pay for software if AI can get these things done faster for me? Now even people who can't code can use Claude to replace existing workflows."

The problem lies in the software industry's business model: **per-seat pricing.**

Traditional logic: You have 100 employees who need CRM, so buy 100 Salesforce accounts.

AI era logic: You have 1 Claude that can do the work of 10 people... how many accounts do you need?

This isn't a question of "efficiency improvement." This is a question of whether the entire business model can survive.

When a $100/month Claude subscription can do what Thomson Reuters and LexisNexis have been selling for decades, how will the market reprice things?

The answer was in that day's stock prices: Thomson Reuters dropped 15.83%, LegalZoom dropped 20%.

---

## So What Exactly Did Karpathy Predict?

Looking back, everything he said is happening:

**First, AI doesn't need to live in the physical world.** Claude lives in a "symbolic world" made of documents, code, and APIs, and still learned to do the work of human experts.

**Second, verifiable environments are key.** His "RLVR" concept—learning in environments where results can be verified—is exactly how Claude Code and Cowork operate. If the code runs, it's right; if it doesn't run, it's wrong. No gray areas.

**Third, AI doesn't need to be a super-brain.** It just needs to be a smart tool user. Claude's Skills architecture proves this.

Wall Street voted to certify this theory with an $830 billion drop.

When the smartest money starts acting on a technical framework, that framework is no longer just academic discussion.

It's reality unfolding.

---

## Afterword

Some say this crash was an overreaction. Maybe.

In the short term, companies won't overnight replace systems they've used for over a decade. Inertia will protect existing players for a while.

But that day's plunge delivered a message:

**The market believes that ghost living in the terminal has truly arrived.**
