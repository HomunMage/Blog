---
title:  "Everyone Uses RLVR. So Why Is Claude Winning? The Answer Changes How You Think About AI's Future"
date:   2026-02-15 10:00:00 +0800
tags: [Tech]
---


## The Question Nobody's Asking

In my previous articles, I argued that RLVR (Reinforcement Learning with Verifiable Rewards) is the key to AI intelligence, and that Claude Code CLI proves Karpathy's "Ghost, not Creature" thesis.

But after publishing, a question kept nagging me:

If RLVR is the answer, why isn't everyone winning equally?

OpenAI does RLVR. Google DeepMind does RLVR. Meta does RLVR. Every major AI lab knows this is the right direction.

So why does Claude keep pulling ahead in the one area that actually matters—getting real work done?

This question forced me to dig deeper. And the answer I found changes how I think about the entire AI landscape.

## Wait—Isn't Everyone Doing the Same Thing?

Let's be honest about what's happening in the industry right now.

OpenAI's o1 and o3 models use reinforcement learning with chain-of-thought reasoning. Google's Gemini uses RL-based post-training. Meta's Llama uses RLHF. Everyone read the same papers. Everyone knows verifiable environments matter.

This is like saying "every restaurant uses heat to cook food." True. But completely unhelpful for explaining why one restaurant gets three Michelin stars and another gets shut down by the health inspector.

RLVR is a necessary condition for building great AI. It is not a sufficient condition.

So what's the sufficient condition?

## The Real Answer: It's Not the Method, It's What You Verify

RLVR has three letters that matter: Reinforcement Learning with **Verifiable** Rewards.

Everyone focuses on the RL part. But the real differentiator is the **V**—what exactly are you verifying?

Here's what each company chose to verify:

**OpenAI optimized for:**
- Chat experience (ChatGPT needed to feel natural and engaging)
- Math and science benchmarks (o1/o3 chasing leaderboard scores)
- Verification signal = benchmark scores + user thumbs up/down
- Result: **Excellent at math reasoning, great chat experience, but not the best at agentic tasks**

**Google DeepMind optimized for:**
- Multimodal understanding (video, images, ultra-long context)
- Integration with Google's ecosystem (Search, Workspace, Android)
- Verification signal = multimodal benchmarks + product metrics
- Result: **Best-in-class multimodal and long context, but tool use isn't the strongest**

**Anthropic optimized for:**
- Instruction following and agentic capability from day one
- Constitutional AI trained the model to carefully follow complex, layered rules
- Claude Code generated massive real-world agentic traces for feedback
- Verification signal = real developers succeeding or failing at real tasks in real codebases
- Result: **The best agent, the best at doing actual work**

See the difference?

Everyone's doing RLVR. But Anthropic is verifying against **real-world agentic task completion**, while others are verifying against **benchmarks, chat quality, or multimodal understanding**.

When the market suddenly decided it wanted AI that can do real work—not just chat or ace tests—Anthropic was already miles ahead.

## Three Gym Bros, Three Results

Here's the simplest way I can explain this.

Three people all know that "weight training makes you strong":

**Lifter A (OpenAI):** Trains for powerlifting competitions. Optimizes deadlift, squat, bench press. Crushes every competition. Wins gold medals on benchmark day.

**Lifter B (Google):** Trains for the decathlon. Does a bit of everything—running, jumping, throwing, lifting. Good at everything, best at nothing specific.

**Lifter C (Anthropic):** Trains for construction work. Carries bricks, climbs scaffolding, operates equipment. Not the most impressive in the gym, but unstoppable on the job site.

All three are doing weight training. But when the job posting says "need someone who can actually build things," Lifter C gets hired every time.

This is exactly what happened with SaaSpocalypse. The market didn't need an AI that aces math olympiads. It needed an AI that can review contracts, analyze financials, and write production code.

Anthropic had been training for that job all along.

## Constitutional AI: The Accident That Changed Everything

Here's the part that I find most fascinating.

Anthropic's Constitutional AI was designed for **safety**. The idea was simple: instead of training the model with human feedback on every output, give the model a set of principles (a "constitution") and train it to evaluate and correct itself against those principles.

This was a safety technique. But it had a massive unintended side effect.

Think about what Constitutional AI actually trains the model to do:

```
Read a complex set of rules (the constitution)
→ Understand what each rule means in context
→ Evaluate whether an output follows ALL the rules simultaneously
→ Self-correct when it doesn't
→ Repeat until compliant
```

Now think about what an agentic AI needs to do:

```
Read a complex task description from the user
→ Understand what needs to be done in context
→ Break it into steps and evaluate each step
→ Self-correct when something fails
→ Repeat until the task is complete
```

**It's the same skill.**

Constitutional AI accidentally trained Claude to be the perfect agent. The ability to "carefully understand and follow complex, multi-layered instructions" is exactly what you need for agentic work.

Compare this with OpenAI's approach:

- **RLHF with human preferences** → Model learns to "sound good to humans" → Great for chat
- **Constitutional AI with principles** → Model learns to "follow rules precisely" → Great for agentic tasks

The first makes a charming conversationalist. The second makes a reliable worker.

## The Flywheel Nobody Talks About

There's one more factor that makes the gap self-reinforcing: data flywheels.

```
Claude Code launches → Developers start using it daily
→ Millions of real-world agentic interactions generated
→ Anthropic sees what works and what fails in real codebases
→ Uses this data to improve the model
→ Claude Code gets better → More developers switch to it
→ Even more data → Even better model → ...
```

**OpenAI's flywheel** spins on chat data (hundreds of millions of ChatGPT users). So their chat experience keeps getting better.

**Anthropic's flywheel** spins on agentic coding data (Claude Code users). So their agentic capability keeps getting better.

Where you start the flywheel determines where it takes you.

And here's the uncomfortable truth for competitors: once a flywheel is spinning fast enough, it's incredibly hard to catch up. OpenAI can't just "decide" to be better at agentic tasks—they need the data, and the data comes from users, and users go where the best agent already is.

## Deep Research: The Third Ghost

Now let me connect all of this to something that's been on my mind: AI-generated data.

There's a well-known problem in AI: if you train models on AI-generated output, the distribution collapses. Each generation loses information from the tails. Variance decreases. Eventually you get a degenerate distribution that only produces bland, homogeneous content.

This is called **model collapse**, and it's a real threat as AI-generated content floods the internet.

But here's what's interesting: **Deep Research doesn't suffer from this problem.**

Why?

Because Deep Research is RLVR in the knowledge domain.

Think about it:

| | Claude Code | Deep Research |
|---|---|---|
| Environment | File system + Terminal | Web + Papers + Databases |
| Action | Write code, modify files | Search, query, cross-reference |
| Verification | Does the code compile? Do tests pass? | Do sources support the claim? |
| Feedback | Error messages, test results | Contradictory data, new sources |
| Output | Working software | Synthesized knowledge |

Regular AI generation is a **closed system**. The model generates from its own distribution, with no external verification, no new information coming in. Entropy can only decrease.

Deep Research is an **open system**. Every search query brings in new information from the real world. Every source acts as a verification checkpoint. The system's effective information content can actually **increase**.

This is the same pattern as Claude Code in the terminal—but applied to the knowledge domain instead of the code domain.

In my Ghost framework: **Deep Research is a Ghost that lives in the Knowledge World, and the Web is its Terminal.**

## Why AI-Generated Data Collapses—And Why Deep Research Doesn't

Let me make this concrete.

**Regular AI generation:**
```
Model → Output → No verification → No new information
→ Next generation trains on this output
→ Tail distribution lost → Variance decreases
→ Repeat → Distribution collapses
= Closed system, entropy only goes down
```

It's like a photocopier copying its own copies. Each generation gets blurrier, loses detail.

**Deep Research:**
```
Model → Generate hypothesis → Search external sources
→ Sources support or contradict → Adjust understanding
→ Generate better hypothesis → Search again with refined query
→ Discover unexpected connections → Explore new direction
→ Synthesize across all sources → Produce output grounded in real data
= Open system, entropy can increase
```

Each round of the loop brings in information that **didn't exist in the model's training data**. Paper A and Paper B might never have been connected by any human author, but Deep Research puts them in the same context window and discovers a novel synthesis.

This isn't hallucination. It's **legitimate combinatorial creativity**—the same thing human researchers do in literature reviews, just at a speed and breadth no human can match.

And crucially, every claim is grounded in retrievable sources. The "V" in RLVR here is source verification. If the sources don't support it, the model has to revise.

## The Missing Piece: Three Ghosts, Three Worlds

Looking at everything together, I now see a unified framework:

**Ghost 1: Claude Code**
- Lives in: The Code World
- Verification: Compilation + Test Results
- Superpower: Can write and debug software better than most human developers
- Already proven: SaaSpocalypse showed the market impact

**Ghost 2: Deep Research**
- Lives in: The Knowledge World
- Verification: Source Grounding + Cross-Reference Consistency
- Superpower: Can synthesize knowledge across domains better than any individual expert
- Key insight: Breaks the model collapse problem through continuous external information injection

**Ghost 3: AI Agents in Business** (emerging)
- Lives in: The Social/Business World
- Verification: Client satisfaction, contract correctness, financial accuracy
- Superpower: Can handle professional services work (legal, financial, sales)
- This is what Claude Cowork represents

All three share the same underlying principle:

> You don't need a world model. You don't need a physical body. You just need a **verifiable environment** and an **RLVR loop**. The Ghost can develop genuine intelligence in any symbolic domain where actions have consequences and consequences can be checked.

And here's the key realization: **the reason AI-generated data causes dimension collapse is precisely because it lacks the "V."** There's no verification, no grounding, no external information. It's a closed loop.

Any system that introduces real verification—whether through code compilation, source grounding, or real-world business outcomes—breaks the collapse and enables genuine learning.

## What This Means for the Future

So if everyone knows RLVR is the way, what happens next?

**Short term:** Other labs will try to spin up their own agentic flywheels. OpenAI has Codex. Google has Jules. They'll close the gap somewhat, but Anthropic's head start in real-world agentic data is significant.

**Medium term:** The battle shifts from "which model is smartest" to "which model has the best verifiable environments." The company that can create the most diverse, highest-quality verification signals wins.

**Long term:** The Three Ghosts converge. An AI that can write code AND do deep research AND handle business tasks—all with proper verification at each layer—becomes something that's hard to distinguish from general intelligence.

But here's my contrarian prediction:

**The winner won't be whoever builds the biggest model. It'll be whoever builds the best verification infrastructure.**

Because in the RLVR framework, the quality of your "V" determines everything. A smaller model with better verification signals will beat a larger model with worse ones. Every time.

This is exactly what we've already seen: Claude isn't the biggest model. But it has the best verification loop for the tasks that matter.

## Afterword

When I wrote my first article about Claude Code CLI, I thought RLVR was a binary thing—either you use it or you don't. That was wrong.

RLVR is more like cooking. Everyone uses heat. But the ingredients you choose, the temperature you set, the timing you follow, the combinations you discover—that's what separates a street vendor from a three-star chef.

Anthropic chose to verify against real-world agentic task completion. Constitutional AI accidentally gave them the perfect foundation for instruction following. Claude Code's flywheel gave them data nobody else has.

That's not a "better method." That's a **better strategic bet** that happened to be exactly right for this moment in AI history.

The question for everyone else isn't "should we do RLVR?" They already are.

The question is: "What are we verifying against, and is it the right thing?"

---

*正解はない、あるのは妥協だけだ。*
